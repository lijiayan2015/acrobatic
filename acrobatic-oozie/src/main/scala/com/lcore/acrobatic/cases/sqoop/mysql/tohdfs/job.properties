nameNode=hdfs://vhost1:9000
jobTracker=vhost1
queueName=launcher

oozie.use.system.libpath=true

oozie.wf.application.path=${nameNode}/oozie/apps/sqoop/tohdfs


# 参数说明
#
# –append
# 将数据追加到hdfs中已经存在的dataset中。使用该参数，sqoop将把数据先导入到一个临时目录中，然后重新给文件命名到一个正式的目录中，以避免和该目录中已存在的文件重名。
#
#
# –as-avrodatafile
# 将数据导入到一个Avro数据文件中
#
#
# –as-sequencefile
# 将数据导入到一个sequence文件中
#
#
# –as-textfile
# 将数据导入到一个普通文本文件中，生成该文本文件后，可以在hive中通过sql语句查询出结果。
#
#
# –boundary-query
# 边界查询，也就是在导入前先通过SQL查询得到一个结果集，然后导入的数据就是该结果集内的数据，格式如：–boundary-query ‘select id,no from t where id = 3’，表示导入的数据为id=3的记录，或者 select min(), max() from ，注意查询的字段中不能有数据类型为字符串的字段，否则会报错
#
#
# –columns
# 指定要导入的字段值，格式如：–columns id,username
#
#
# –direct
# 直接导入模式，使用的是关系数据库自带的导入导出工具。官网上是说这样导入会更快
#
#
# –direct-split-size
# 在使用上面direct直接导入的基础上，对导入的流按字节数分块，特别是使用直连模式从PostgreSQL导入数据的时候，可以将一个到达设定大小的文件分为几个独立的文件。


# –inline-lob-limit
# 设定大对象数据类型的最大值
#
#
# -m,–num-mappers
# 启动N个map来并行导入数据，默认是4个，最好不要将数字设置为高于集群的节点数
#
#
# –query，-e
# 从查询结果中导入数据，该参数使用时必须指定–target-dir、–hive-table，在查询语句中一定要有where条件且在where条件中需要包含 $CONDITIONS，示例：–query ‘select * from t where $CONDITIONS ’ –target-dir /tmp/t –hive-table t
#
#
# –split-by
# 表的列名，用来切分工作单元，一般后面跟主键ID
#
#
# –table
# 关系数据库表名，数据从该表中获取
#
#
# –delete-target-dir
# 删除目标目录
#
# –target-dir
# 指定hdfs路径
#
#
# –warehouse-dir
# 与 –target-dir 不能同时使用，指定数据导入的存放目录，适用于hdfs导入，不适合导入hive目录


# –where
# 从关系数据库导入数据时的查询条件，示例：–where “id = 2”
#
#
# -z,–compress
# 压缩参数，默认情况下数据是没被压缩的，通过该参数可以使用gzip压缩算法对数据进行压缩，适用于SequenceFile, text文本文件, 和Avro文件
#
#
# –compression-codec
# Hadoop压缩编码，默认是gzip
#
#
# –null-string
# 可选参数，如果没有指定，则字符串null将被使用
#
#
# –null-non-string
# 可选参数，如果没有指定，则字符串null将被使用
#

# --fields-terminated-by 指定列之前的分隔符



# 错误：
# 2、-m 表示并发启动map的数量 -m 1表示启动一个map, 指定-m > 1时，必须添加 --split-by 指定分割列，，
# 分割列要求至少满足两个条件中任一个：1）目标表有主键   2）有num 类型或者date类型，
# 因为会执行 min(split_column)，max(split_column)操作，决定如何分割否则无法启用多个map
#
##
## sqoop import \
## --connect jdbc:mysql://vhost1:3306/yss \
## --username root \
## --password root \
## --table workers \
## --target-dir /jobs/oozie/out/sqoop/tohdfs \
## --delete-target-dir \
## --fields-terminated-by '#' \
## --split-by 'workerAge' \
## -m 3
#
# 18/10/11 11:25:38 ERROR tool.ImportTool: Import failed: java.io.IOException: Generating splits for a textual index column allowed only in case of "-Dorg.apache.sqoop.splitter.allow_text_splitter=true" property passed as a parameter
#        at org.apache.sqoop.mapreduce.db.DataDrivenDBInputFormat.getSplits(DataDrivenDBInputFormat.java:204)
#        at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:301)
#        at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:318)
#        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:196)
#        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1290)
#        at org.apache.hadoop.mapreduce.Job$10.run(Job.java:1287)
#        at java.security.AccessController.doPrivileged(Native Method)
#        at javax.security.auth.Subject.doAs(Subject.java:422)
#        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1762)
#        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1287)
#        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1308)
#        at org.apache.sqoop.mapreduce.ImportJobBase.doSubmitJob(ImportJobBase.java:200)
#        at org.apache.sqoop.mapreduce.ImportJobBase.runJob(ImportJobBase.java:173)
#        at org.apache.sqoop.mapreduce.ImportJobBase.runImport(ImportJobBase.java:270)
#        at org.apache.sqoop.manager.SqlManager.importTable(SqlManager.java:692)
#        at org.apache.sqoop.manager.MySQLManager.importTable(MySQLManager.java:127)
#        at org.apache.sqoop.tool.ImportTool.importTable(ImportTool.java:520)
#        at org.apache.sqoop.tool.ImportTool.run(ImportTool.java:628)
#        at org.apache.sqoop.Sqoop.run(Sqoop.java:147)
#        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:70)
#        at org.apache.sqoop.Sqoop.runSqoop(Sqoop.java:183)
#        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:234)
#        at org.apache.sqoop.Sqoop.runTool(Sqoop.java:243)
#        at org.apache.sqoop.Sqoop.main(Sqoop.java:252)
#Caused by: Generating splits for a textual index column allowed only in case of "-Dorg.apache.sqoop.splitter.allow_text_splitter=true" property passed as a parameter
#        at org.apache.sqoop.mapreduce.db.TextSplitter.split(TextSplitter.java:67)
#        at org.apache.sqoop.mapreduce.db.DataDrivenDBInputFormat.getSplits(DataDrivenDBInputFormat.java:201)
#        ... 23 more
#
#[hadoop@vhost1 tohdfs]$

#  Failing Oozie Launcher, Main class [org.apache.oozie.action.hadoop.SqoopMain],
#   exit code [1]   原因可能是导数据的参数有误，如上面的语句用oozie来导，就会此错
#  用后面的命令则可以成功。import --connect jdbc:mysql://vhost1:3306/yss --username root --password root --table workers --target-dir /jobs/oozie/out/sqoop/tohdfs --delete-target-dir --input-fields-terminated-by '#' -m 1
#







